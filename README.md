# Deep_Learning_basics
1. Deep Learnig book by Ian Goodfellow et al: http://www.deeplearningbook.org/

# Softmax Regression
1. Brandon Rohrer's excellent intro : https://e2eml.school/softmax.html
2. Stanford Tutorial: http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/

# Attention, Self Attention
1. Andrew Ng's Sequence Model videos
2. https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#whats-wrong-with-seq2seq-model
3. Configure Encoder-Decoder models: https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/

# Excellent Implementation Examples from Tensorflow
1. https://www.tensorflow.org/tutorials/

# Learning Rate and Batch Size implications
https://arxiv.org/pdf/1804.07612.pdf

# Learning Rate in little more details
https://www.jeremyjordan.me/nn-learning-rate/#:~:text=When%20entering%20the%20optimal%20learning,even%20diverge%20from%20the%20minima.

# OPtimization fundamentals
https://cs231n.github.io/optimization-1/

# Gradient Descent Algorithms
  1. Andrew Ng's Deep Learning course in deeplearning.ai
  2. https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=2
  3. https://ruder.io/optimizing-gradient-descent/
  4. Geoff Hinton's slides: http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf

# Gradient Checking
  1. Andrew Ng's deeplearning video
  2. Stanford Notes: https://cs231n.github.io/neural-networks-3/#gradcheck
